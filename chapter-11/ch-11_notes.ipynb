{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import InputLayer\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import os \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7f8860da6850>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change initializer in hopes of speeding training and avoiding exploding or disapearing gradients\n",
    "Dense(10, activation='relu', kernel_initializer='he_normal')\n",
    "# different one\n",
    "he_avg_init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg', distribution='uniform')\n",
    "Dense(10, activation='sigmoid', kernel_initializer=he_avg_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# typical preformance SELU > ELU > leaky ReLU > ReLU > tanh > logistic\n",
    "# adding a leaky ReLU activation function\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    # model input \n",
    "    Dense(10, kernel_initializer='he_normal'),\n",
    "    keras.layers.LeakyReLU(alpha=0.2)\n",
    "    # rest of model\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for selu you need a special initial conditions\n",
    "layers = Dense(10, activation='selu', kernel_initializer='lecun_normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using batch normalization on images\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    Flatten(input_shape=[28,28]),\n",
    "    BatchNormalization(),\n",
    "    Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "    BatchNormalization(),\n",
    "    Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    BatchNormalization(),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization/gamma:0', True),\n",
       " ('batch_normalization/beta:0', True),\n",
       " ('batch_normalization/moving_mean:0', False),\n",
       " ('batch_normalization/moving_variance:0', False)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(var.name, var.trainable) for var in model.layers[1].variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if putting BN after layer you need to seperate the activation function\n",
    "model = keras.models.Sequential([\n",
    "    Flatten(input_shape=[28,28]),\n",
    "    BatchNormalization(),\n",
    "    Dense(300, use_bias=False, kernel_initializer='he_normal'),\n",
    "    BatchNormalization(),\n",
    "    Activation('elu'),\n",
    "    Dense(100, use_bias=False, kernel_initializer='he_normal'),\n",
    "    BatchNormalization(),\n",
    "    Activation('elu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 300)               235200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               30000     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 270,946\n",
      "Trainable params: 268,578\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other approach it so do gradient clipping to avoid exploding gradient\n",
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
    "model.compile(loss='mse', optimizer=optimizer)\n",
    "\n",
    "# if you want to preserve direction, but still clip\n",
    "optimizer = keras.optimizers.SGD(clipvalue=1.0, clipnorm=1.0)\n",
    "model.compile(loss='mse', optimizer=optimizer)\n",
    "model.save('my_model_A.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer learning assuming you have some pretrained model \"my_model_A.h5\"\n",
    "\n",
    "model_A = keras.models.load_model('my_model_A.h5')\n",
    "# clone model A, so its layers are not changed\n",
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())\n",
    "# add all but ouput layer\n",
    "model_B_on_A = keras.models.Sequential(model_A_clone.layers[:-1])\n",
    "model_B_on_A.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set all non output layers to non-trainable to aviod destroying the old model\n",
    "# before the output layer is okay\n",
    "\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model_B_on_A.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading some data\n",
    "split_idx = 10000\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train, x_valid = x_train[split_idx:] / 255.0, x_train[:split_idx] / 255.0\n",
    "y_train, y_valid = y_train[split_idx:], y_train[:split_idx]\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: -18845.2324 - accuracy: 0.1123 - val_loss: -19864.2363 - val_accuracy: 0.1127\n",
      "Epoch 2/4\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: -21062.3262 - accuracy: 0.1123 - val_loss: -22071.2051 - val_accuracy: 0.1127\n",
      "Epoch 3/4\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: -23279.3711 - accuracy: 0.1123 - val_loss: -24278.3848 - val_accuracy: 0.1127\n",
      "Epoch 4/4\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: -25496.5957 - accuracy: 0.1123 - val_loss: -26485.5254 - val_accuracy: 0.1127\n",
      "Epoch 1/16\n",
      "1563/1563 [==============================] - 3s 1ms/step - loss: -26554.9549 - accuracy: 0.1138 - val_loss: -26507.6016 - val_accuracy: 0.1127\n",
      "Epoch 2/16\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: -26506.7608 - accuracy: 0.1138 - val_loss: -26529.6777 - val_accuracy: 0.1127\n",
      "Epoch 3/16\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: -26500.8520 - accuracy: 0.1140 - val_loss: -26551.7383 - val_accuracy: 0.1127\n",
      "Epoch 4/16\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: -26719.2064 - accuracy: 0.1111 - val_loss: -26573.8184 - val_accuracy: 0.1127\n",
      "Epoch 5/16\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: -26772.1599 - accuracy: 0.1106 - val_loss: -26595.8887 - val_accuracy: 0.1127\n",
      "Epoch 6/16\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: -26689.1949 - accuracy: 0.1123 - val_loss: -26617.9688 - val_accuracy: 0.1127\n",
      "Epoch 7/16\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: -26637.5394 - accuracy: 0.1125 - val_loss: -26640.0312 - val_accuracy: 0.1127\n",
      "Epoch 8/16\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: -26571.7899 - accuracy: 0.1133 - val_loss: -26662.1113 - val_accuracy: 0.1127\n",
      "Epoch 9/16\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: -26838.0877 - accuracy: 0.1127 - val_loss: -26684.1895 - val_accuracy: 0.1127\n",
      "Epoch 10/16\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: -26717.7037 - accuracy: 0.1123 - val_loss: -26706.2617 - val_accuracy: 0.1127\n",
      "Epoch 11/16\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: -26919.1212 - accuracy: 0.1121 - val_loss: -26728.3398 - val_accuracy: 0.1127\n",
      "Epoch 12/16\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: -26832.2539 - accuracy: 0.1129 - val_loss: -26750.4082 - val_accuracy: 0.1127\n",
      "Epoch 13/16\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: -26892.5206 - accuracy: 0.1135 - val_loss: -26772.4883 - val_accuracy: 0.1127\n",
      "Epoch 14/16\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: -26852.8923 - accuracy: 0.1142 - val_loss: -26794.5547 - val_accuracy: 0.1127\n",
      "Epoch 15/16\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: -26858.4066 - accuracy: 0.1130 - val_loss: -26816.6465 - val_accuracy: 0.1127\n",
      "Epoch 16/16\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: -26907.7827 - accuracy: 0.1118 - val_loss: -26838.7188 - val_accuracy: 0.1127\n"
     ]
    }
   ],
   "source": [
    "# train for a few epochos and then unfreezing layers\n",
    "\n",
    "history = model_B_on_A.fit(x_train, y_train, epochs=4, validation_data=(x_valid, y_valid))\n",
    "\n",
    "for layers in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "    \n",
    "# lower learning rate to avoid destroying old weights\n",
    "optimizer = keras.optimizers.SGD(lr=1e-4) # was 1e-2\n",
    "model_B_on_A.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "history = model_B_on_A.fit(x_train, y_train, epochs=16, validation_data=(x_valid, y_valid))\n",
    "\n",
    "# this does not work super well for dense DNN, but it will be revisited for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
